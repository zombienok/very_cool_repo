{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW07: Кластеризация, внутренние метрики качества, PCA/t-SNE\n",
    "\n",
    "## Задание\n",
    "Цель: Закрепить понимание различий между семействами методов кластеризации, навыки корректного препроцессинга для distance-based методов, оценку качества кластеризации без истинных меток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорты\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Установка стиля для графиков\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасетов\n",
    "datasets = []\n",
    "dataset_names = ['S07-hw-dataset-01', 'S07-hw-dataset-02', 'S07-hw-dataset-03']\n",
    "\n",
    "for name in dataset_names:\n",
    "    df = pd.read_csv(f'data/{name}.csv')\n",
    "    datasets.append(df)\n",
    "    print(f\"Dataset {name} shape: {df.shape}\")\n",
    "    print(f\"Dataset {name} info:\")\n",
    "    print(df.info())\n",
    "    print(f\"Dataset {name} description:\")\n",
    "    print(df.describe())\n",
    "    print(f\"Dataset {name} missing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для препроцессинга каждого датасета\n",
    "def preprocess_dataset(df):\n",
    "    # Сохраняем sample_id отдельно\n",
    "    sample_ids = df['sample_id'].copy()\n",
    "    \n",
    "    # Удаляем sample_id из признаков\n",
    "    X = df.drop(columns=['sample_id'])\n",
    "    \n",
    "    # Определяем типы признаков\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numeric features: {numeric_features}\")\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    \n",
    "    # Создаем препроцессор\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numeric_features),\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "            ]), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Применяем препроцессинг\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    return X_processed, sample_ids, preprocessor, numeric_features, categorical_features\n",
    "\n",
    "# Обработка всех датасетов\n",
    "processed_data = []\n",
    "preprocessors = []\n",
    "\n",
    "for i, df in enumerate(datasets):\n",
    "    print(f\"Processing dataset {i+1}: {dataset_names[i]}\")\n",
    "    X_proc, ids, prep, num_feats, cat_feats = preprocess_dataset(df)\n",
    "    processed_data.append((X_proc, ids))\n",
    "    preprocessors.append(prep)\n",
    "    print(f\"Processed shape: {X_proc.shape}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация и оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для оценки кластеризации\n",
    "def evaluate_clustering(X, labels):\n",
    "    # Убираем точки с меткой -1 (шум для DBSCAN)\n",
    "    mask = labels != -1\n",
    "    X_filtered = X[mask]\n",
    "    labels_filtered = labels[mask]\n",
    "    \n",
    "    if len(np.unique(labels_filtered)) < 2:\n",
    "        return {'silhouette': -1, 'davies_bouldin': -1, 'calinski_harabasz': -1, 'noise_ratio': 1 - len(X_filtered)/len(X)}\n",
    "    \n",
    "    try:\n",
    "        sil_score = silhouette_score(X_filtered, labels_filtered)\n",
    "    except:\n",
    "        sil_score = -1\n",
    "    \n",
    "    try:\n",
    "        db_score = davies_bouldin_score(X_filtered, labels_filtered)\n",
    "    except:\n",
    "        db_score = -1\n",
    "    \n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(X_filtered, labels_filtered)\n",
    "    except:\n",
    "        ch_score = -1\n",
    "    \n",
    "    noise_ratio = 1 - len(X_filtered)/len(X)\n",
    "    \n",
    "    return {\n",
    "        'silhouette': sil_score,\n",
    "        'davies_bouldin': db_score,\n",
    "        'calinski_harabasz': ch_score,\n",
    "        'noise_ratio': noise_ratio\n",
    "    }\n",
    "\n",
    "# Функция для кластеризации с подбором параметров\n",
    "def cluster_and_evaluate(X, dataset_idx):\n",
    "    results = {}\n",
    "    \n",
    "    # KMeans\n",
    "    print(f\"\\nDataset {dataset_idx+1}: KMeans\")\n",
    "    k_range = range(2, 21)\n",
    "    kmeans_scores = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        scores = evaluate_clustering(X, labels)\n",
    "        kmeans_scores.append(scores)\n",
    "    \n",
    "    # Находим оптимальное k по silhouette\n",
    "    sil_scores = [score['silhouette'] for score in kmeans_scores]\n",
    "    best_k_idx = np.argmax(sil_scores)\n",
    "    best_k = k_range[best_k_idx]\n",
    "    \n",
    "    # Обучаем KMeans с лучшим k\n",
    "    kmeans_best = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "    kmeans_labels = kmeans_best.fit_predict(X)\n",
    "    \n",
    "    results['kmeans'] = {\n",
    "        'labels': kmeans_labels,\n",
    "        'params': {'n_clusters': best_k},\n",
    "        'scores': evaluate_clustering(X, kmeans_labels),\n",
    "        'silhouette_by_k': sil_scores\n",
    "    }\n",
    "    \n",
    "    # DBSCAN\n",
    "    print(f\"Dataset {dataset_idx+1}: DBSCAN\")\n",
    "    eps_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    min_samples_values = [2, 3, 4, 5, 6, 7, 8]\n",
    "    \n",
    "    dbscan_results = []\n",
    "    param_combinations = []\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            try:\n",
    "                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                labels = dbscan.fit_predict(X)\n",
    "                scores = evaluate_clustering(X, labels)\n",
    "                dbscan_results.append(scores)\n",
    "                param_combinations.append({'eps': eps, 'min_samples': min_samples})\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Находим лучшую комбинацию параметров\n",
    "    if dbscan_results:\n",
    "        sil_scores_dbscan = [score['silhouette'] for score in dbscan_results]\n",
    "        best_dbscan_idx = np.argmax(sil_scores_dbscan)\n",
    "        best_dbscan_params = param_combinations[best_dbscan_idx]\n",
    "        \n",
    "        dbscan_best = DBSCAN(**best_dbscan_params)\n",
    "        dbscan_labels = dbscan_best.fit_predict(X)\n",
    "        \n",
    "        results['dbscan'] = {\n",
    "            'labels': dbscan_labels,\n",
    "            'params': best_dbscan_params,\n",
    "            'scores': evaluate_clustering(X, dbscan_labels)\n",
    "        }\n",
    "    else:\n",
    "        results['dbscan'] = None\n",
    "    \n",
    "    # Agglomerative Clustering\n",
    "    print(f\"Dataset {dataset_idx+1}: Agglomerative Clustering\")\n",
    "    linkages = ['ward', 'complete', 'average', 'single']\n",
    "    agg_results = {}\n",
    "    \n",
    "    for linkage in linkages:\n",
    "        agg_scores = []\n",
    "        for k in k_range:\n",
    "            try:\n",
    "                if linkage == 'ward':\n",
    "                    agg = AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
    "                else:\n",
    "                    # Для других linkage нужно использовать евклидово расстояние\n",
    "                    agg = AgglomerativeClustering(n_clusters=k, linkage=linkage, metric='euclidean')\n",
    "                labels = agg.fit_predict(X)\n",
    "                scores = evaluate_clustering(X, labels)\n",
    "                agg_scores.append(scores)\n",
    "            except:\n",
    "                agg_scores.append({'silhouette': -1, 'davies_bouldin': -1, 'calinski_harabasz': -1, 'noise_ratio': 1})\n",
    "        \n",
    "        agg_results[linkage] = {\n",
    "            'silhouette_by_k': [score['silhouette'] for score in agg_scores],\n",
    "            'scores_by_k': agg_scores\n",
    "        }\n",
    "    \n",
    "    # Выбираем лучший linkage по максимальному silhouette\n",
    "    best_linkage = max(linkages, key=lambda l: max(agg_results[l]['silhouette_by_k']))\n",
    "    best_agg_k_idx = np.argmax(agg_results[best_linkage]['silhouette_by_k'])\n",
    "    best_agg_k = k_range[best_agg_k_idx]\n",
    "    \n",
    "    agg_best = AgglomerativeClustering(n_clusters=best_agg_k, linkage=best_linkage)\n",
    "    agg_labels = agg_best.fit_predict(X)\n",
    "    \n",
    "    results['agglomerative'] = {\n",
    "        'labels': agg_labels,\n",
    "        'params': {'n_clusters': best_agg_k, 'linkage': best_linkage},\n",
    "        'scores': evaluate_clustering(X, agg_labels),\n",
    "        'results_by_linkage': agg_results\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кластеризация для каждого датасета\n",
    "all_results = []\n",
    "\n",
    "for i, (X, sample_ids) in enumerate(processed_data):\n",
    "    print(f\"\\nProcessing Dataset {i+1}: {dataset_names[i]}\")\n",
    "    results = cluster_and_evaluate(X, i)\n",
    "    all_results.append(results)\n",
    "    print(f\"Completed Dataset {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания визуализаций\n",
    "def create_visualizations(all_results, processed_data, dataset_names):\n",
    "    # Создаем PCA визуализации для лучшего результата каждого датасета\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    for i, (results, (X, _)) in enumerate(zip(all_results, processed_data)):\n",
    "        # PCA для лучшего результата\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        # Определяем лучший метод по silhouette\n",
    "        best_method = max(['kmeans', 'dbscan', 'agglomerative'], \n",
    "                         key=lambda m: results[m]['scores']['silhouette'] if results[m] and results[m]['scores']['silhouette'] != -1 else -2)\n",
    "        \n",
    "        best_labels = results[best_method]['labels']\n",
    "        \n",
    "        # График PCA с кластерами\n",
    "        scatter = axes[0, i].scatter(X_pca[:, 0], X_pca[:, 1], c=best_labels, cmap='viridis', alpha=0.7)\n",
    "        axes[0, i].set_title(f'{dataset_names[i]} - Best Method: {best_method}')\n",
    "        axes[0, i].set_xlabel('PCA Component 1')\n",
    "        axes[0, i].set_ylabel('PCA Component 2')\n",
    "        \n",
    "        # Подбор параметров для KMeans (silhouette vs k)\n",
    "        k_range = range(2, 21)\n",
    "        axes[1, i].plot(k_range, results['kmeans']['silhouette_by_k'], marker='o', label='Silhouette Score')\n",
    "        axes[1, i].set_title(f'{dataset_names[i]} - KMeans Silhouette vs K')\n",
    "        axes[1, i].set_xlabel('Number of Clusters (K)')\n",
    "        axes[1, i].set_ylabel('Silhouette Score')\n",
    "        axes[1, i].grid(True)\n",
    "        axes[1, i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/figures/pca_and_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Визуализация для Agglomerative Clustering по разным linkage\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    for i, results in enumerate(all_results):\n",
    "        agg_results = results['agglomerative']['results_by_linkage']\n",
    "        k_range = range(2, 21)\n",
    "        \n",
    "        for j, linkage in enumerate(['ward', 'complete']):\n",
    "            ax = axes[j, i]\n",
    "            ax.plot(k_range, agg_results[linkage]['silhouette_by_k'], marker='o', label=f'{linkage} linkage')\n",
    "            ax.set_title(f'{dataset_names[i]} - {linkage} Linkage')\n",
    "            ax.set_xlabel('Number of Clusters (K)')\n",
    "            ax.set_ylabel('Silhouette Score')\n",
    "            ax.grid(True)\n",
    "            ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/figures/agglomerative_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Создаем визуализации\n",
    "create_visualizations(all_results, processed_data, dataset_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка устойчивости (на одном датасете)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка устойчивости для KMeans на первом датасете\n",
    "def stability_check(X, n_runs=5):\n",
    "    all_labels = []\n",
    "    \n",
    "    # Определяем оптимальное количество кластеров\n",
    "    k_range = range(2, 11)\n",
    "    best_k = 2  # Начальное значение\n",
    "    best_silhouette = -1\n",
    "    \n",
    "    for k in k_range:\n",
    "        sil_scores = []\n",
    "        for run in range(3):  # Для ускорения используем 3 прогона\n",
    "            kmeans = KMeans(n_clusters=k, random_state=run, n_init=10)\n",
    "            labels = kmeans.fit_predict(X)\n",
    "            if len(np.unique(labels)) > 1:\n",
    "                try:\n",
    "                    sil = silhouette_score(X, labels)\n",
    "                    sil_scores.append(sil)\n",
    "                except:\n",
    "                    sil_scores.append(-1)\n",
    "            else:\n",
    "                sil_scores.append(-1)\n",
    "        \n",
    "        avg_sil = np.mean([s for s in sil_scores if s != -1])\n",
    "        if avg_sil > best_silhouette:\n",
    "            best_silhouette = avg_sil\n",
    "            best_k = k\n",
    "    \n",
    "    print(f\"Best k for stability test: {best_k}\")\n",
    "    \n",
    "    # Запускаем KMeans с разными random_state\n",
    "    for run in range(n_runs):\n",
    "        kmeans = KMeans(n_clusters=best_k, random_state=run*42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    # Рассчитываем ARI между всеми парами\n",
    "    n_pairs = 0\n",
    "    total_ari = 0\n",
    "    \n",
    "    for i in range(len(all_labels)):\n",
    "        for j in range(i+1, len(all_labels)):\n",
    "            ari = adjusted_rand_score(all_labels[i], all_labels[j])\n",
    "            total_ari += ari\n",
    "            n_pairs += 1\n",
    "    \n",
    "    avg_ari = total_ari / n_pairs if n_pairs > 0 else 0\n",
    "    print(f\"Average ARI between runs: {avg_ari:.3f}\")\n",
    "    \n",
    "    return avg_ari, all_labels\n",
    "\n",
    "# Проверяем устойчивость на первом датасете\n",
    "print(\"Stability Check for Dataset 1 (KMeans):\")\n",
    "stability_ari, stability_labels = stability_check(processed_data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение артефактов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем метрики\n",
    "metrics_summary = []\n",
    "best_configs = []\n",
    "\n",
    "for i, (results, (_, sample_ids)) in enumerate(zip(all_results, processed_data)):\n",
    "    dataset_metrics = {\n",
    "        'dataset': dataset_names[i],\n",
    "        'kmeans': results['kmeans']['scores'],\n",
    "        'dbscan': results['dbscan']['scores'] if results['dbscan'] else None,\n",
    "        'agglomerative': results['agglomerative']['scores']\n",
    "    }\n",
    "    metrics_summary.append(dataset_metrics)\n",
    "    \n",
    "    # Сохраняем лучшие конфигурации\n",
    "    # Определяем лучший метод по silhouette\n",
    "    methods_scores = {}\n",
    "    for method in ['kmeans', 'dbscan', 'agglomerative']:\n",
    "        if results[method] and results[method]['scores']['silhouette'] != -1:\n",
    "            methods_scores[method] = results[method]['scores']['silhouette']\n",
    "    \n",
    "    if methods_scores:\n",
    "        best_method = max(methods_scores, key=methods_scores.get)\n",
    "        best_config = {\n",
    "            'dataset': dataset_names[i],\n",
    "            'best_method': best_method,\n",
    "            'best_params': results[best_method]['params'],\n",
    "            'best_scores': results[best_method]['scores']\n",
    "        }\n",
    "        best_configs.append(best_config)\n",
    "    \n",
    "    # Сохраняем метки для лучшего решения\n",
    "    if methods_scores:\n",
    "        best_method = max(methods_scores, key=methods_scores.get)\n",
    "        best_labels = results[best_method]['labels']\n",
    "        \n",
    "        labels_df = pd.DataFrame({\n",
    "            'sample_id': sample_ids,\n",
    "            'cluster_label': best_labels\n",
    "        })\n",
    "        labels_df.to_csv(f'artifacts/labels_hw07_ds{i+1}.csv', index=False)\n",
    "\n",
    "# Сохраняем JSON файлы\n",
    "with open('artifacts/metrics_summary.json', 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\n",
    "with open('artifacts/best_configs.json', 'w') as f:\n",
    "    json.dump(best_configs, f, indent=2)\n",
    "\n",
    "print(\"Artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги по каждому датасету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (results, _) in enumerate(zip(all_results, processed_data)):\n",
    "    print(f\"\\nDataset {i+1} ({dataset_names[i]}):\")\n",
    "    \n",
    "    # Определяем лучший метод\n",
    "    methods_scores = {}\n",
    "    for method in ['kmeans', 'dbscan', 'agglomerative']:\n",
    "        if results[method] and results[method]['scores']['silhouette'] != -1:\n",
    "            methods_scores[method] = results[method]['scores']\n",
    "    \n",
    "    if methods_scores:\n",
    "        best_method = max(methods_scores, key=lambda m: methods_scores[m]['silhouette'])\n",
    "        best_scores = methods_scores[best_method]\n",
    "        \n",
    "        print(f\"  Лучший метод: {best_method}\")\n",
    "        print(f\"  Silhouette Score: {best_scores['silhouette']:.3f}\")\n",
    "        print(f\"  Davies-Bouldin Score: {best_scores['davies_bouldin']:.3f}\")\n",
    "        print(f\"  Calinski-Harabasz Score: {best_scores['calinski_harabasz']:.3f}\")\n",
    "        print(f\"  Доля шума: {best_scores['noise_ratio']:.3f}\")\n",
    "        \n",
    "        # Краткий анализ\n",
    "        print(f\"  Анализ:\")\n",
    "        if best_method == 'kmeans':\n",
    "            print(f\"    - KMeans показал хорошие результаты, что говорит о наличии компактных кластеров\")\n",
    "        elif best_method == 'dbscan':\n",
    "            print(f\"    - DBSCAN справился лучше, возможно, данные имеют произвольную форму или содержат шум\")\n",
    "        elif best_method == 'agglomerative':\n",
    "            print(f\"    - Аггломеративная кластеризация подошла, что может говорить о иерархической структуре данных\")\n",
    "    else:\n",
    "        print(\"  Не удалось получить корректные результаты для ни одного метода\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}